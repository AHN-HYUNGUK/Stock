# main.py

import os
import datetime
import requests
import schedule
import time
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import csv, io, json, re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ HTTP ì„¤ì • / ë””ë²„ê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HTTP_HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0 Safari/537.36",
    "Accept": "application/json, text/csv;q=0.9,*/*;q=0.8",
}
HTTP_DEBUG = True  # ë™ì‘ í™•ì¸ í›„ Falseë¡œ ë‚´ë ¤ë„ ë¨

# ì˜ëª»ëœ ì‹œìŠ¤í…œ í”„ë¡ì‹œ ë¬´ì‹œ
for k in ("HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy"):
    os.environ.pop(k, None)
os.environ.setdefault("NO_PROXY", "*")

S = requests.Session()
S.trust_env = False
S.headers.update(HTTP_HEADERS)
_DEF_PROXIES = {"http": None, "https": None}

def _mask_url(u: str) -> str:
    """ë¡œê·¸ì— ë…¸ì¶œë  URLì—ì„œ í† í°/í‚¤ë¥¼ ***ë¡œ ë§ˆìŠ¤í‚¹."""
    try:
        u = re.sub(r'(api\.telegram\.org\/bot)[^\/]+', r'\1***', u)
        u = re.sub(r'(?i)(apikey|api_key|token|access_token)=[^&]+', r'\1=***', u)
    except Exception:
        pass
    return u

def http_get(url, *, params=None, timeout=20):
    if HTTP_DEBUG:
        try:
            from requests.models import PreparedRequest
            pr = PreparedRequest()
            pr.prepare_url(url, params)
            print(f"[HTTP GET] {_mask_url(pr.url)}")
        except Exception:
            print(f"[HTTP GET] {_mask_url(url)} {params if params else ''}")
    r = S.get(url, params=params, timeout=timeout, proxies=_DEF_PROXIES, allow_redirects=True)
    r.raise_for_status()
    return r

def http_post(url, *, data=None, timeout=20):
    if HTTP_DEBUG:
        print(f"[HTTP POST] {_mask_url(url)} (fields: {list((data or {}).keys())})")
    r = S.post(url, data=data, timeout=timeout, proxies=_DEF_PROXIES, allow_redirects=True)
    r.raise_for_status()
    return r

# (dotenv ì•ˆ ì“°ë©´ ê·¸ëŒ€ë¡œ)
load_dotenv = None


# â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOKEN           = os.environ['TOKEN']
CHAT_IDS        = os.environ['CHAT_IDS'].split("7638597712, 7484882811")  # âœ… ì—¬ëŸ¬ ëª… ì‰¼í‘œë¡œ êµ¬ë¶„
EXCHANGE_KEY    = os.environ['EXCHANGEAPI']
TWELVEDATA_API  = os.environ["TWELVEDATA_API"]
TELEGRAM_URL    = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
today           = datetime.datetime.now().strftime('%Yë…„ %mì›” %dì¼')



# â”€â”€ ì§€í‘œ/ì‹œì„¸ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_us_indices():
    url = "https://www.investing.com/indices/major-indices"
    res = http_get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    rows = soup.select("table tbody tr")[:3]
    out = []
    for r in rows:
        try:
            name = r.select_one("td:nth-child(2)").text.strip()
            now  = float(r.select_one("td:nth-child(3)").text.replace(",", ""))
            prev = float(r.select_one("td:nth-child(4)").text.replace(",", ""))
            diff = now - prev
            pct  = diff / prev * 100
            icon = "â–²" if diff > 0 else "â–¼" if diff < 0 else "-"
            out.append(f"{name}: {now:,.2f} {icon}{abs(diff):,.2f} ({pct:+.2f}%)")
        except Exception:
            out.append(f"{name}: ë°ì´í„° ì˜¤ë¥˜")
    return "\n".join(out)

def get_exchange_rates():
    j = http_get(f"https://v6.exchangerate-api.com/v6/{EXCHANGE_KEY}/latest/USD").json()
    rates = j.get("conversion_rates", {})
    return (
        f"USD: 1.00 ê¸°ì¤€\n"
        f"KRW: {rates.get('KRW', 0):.2f}\n"
        f"JPY (100ì—”): {rates.get('JPY', 0) * 100:.2f}\n"
        f"EUR: {rates.get('EUR', 0):.2f}\n"
        f"CNY: {rates.get('CNY', 0):.2f}"
    )

def get_sector_etf_changes(api_key):
    etfs = {"ğŸ’» ê¸°ìˆ ": "XLK", "ğŸ¦ ê¸ˆìœµ": "XLF", "ğŸ’Š í—¬ìŠ¤ì¼€ì–´": "XLV", "âš¡ ì—ë„ˆì§€": "XLE", "ğŸ›’ ì†Œë¹„ì¬": "XLY"}
    out = []
    for name, sym in etfs.items():
        try:
            j = http_get("https://api.twelvedata.com/quote",
                         params={"symbol": sym, "apikey": api_key}).json()
            p = float(j["close"]); c = float(j["change"]); pct = float(j["percent_change"])
            icon = "â–²" if c > 0 else "â–¼" if c < 0 else "-"
            out.append(f"{name}: {p:.2f} {icon}{abs(c):.2f} ({pct:+.2f}%)")
        except Exception:
            out.append(f"{name}: ì •ë³´ ì—†ìŒ")
    return "\n".join(out)

def get_stock_prices(api_key):
    symbols = {
        "Tesla (TSLA)": "TSLA",
        "Nvidia (NVDA)": "NVDA",
        "Apple (AAPL)": "AAPL",
        "Microsoft (MSFT)": "MSFT",
        "Amazon (AMZN)": "AMZN",
        "Meta (META)": "META",
        "Berkshire Hathaway (BRK.B)": "BRK.B"
    }
    out = []
    for name, sym in symbols.items():
        try:
            j = http_get("https://api.twelvedata.com/quote",
                         params={"symbol": sym, "apikey": api_key}).json()
            p = float(j["close"]); c = float(j["change"]); pct = float(j["percent_change"])
            icon = "â–²" if c > 0 else "â–¼" if c < 0 else "-"
            out.append(f"â€¢ {name}: ${p:.2f} {icon}{abs(c):.2f} ({pct:+.2f}%)")
        except Exception:
            out.append(f"â€¢ {name}: ì •ë³´ ì—†ìŒ")
    return "ğŸ“Œ ì£¼ìš” ì¢…ëª© ì‹œì„¸:\n" + "\n".join(out)

def get_korean_stock_price(stock_code, name):
    try:
        url = f"https://finance.naver.com/item/sise.naver?code={stock_code}"
        res = http_get(url)
        soup = BeautifulSoup(res.text, "html.parser")
        price = soup.select_one("strong#_nowVal").text.replace(",", "")
        change = soup.select_one("span#_change").text.strip().replace(",", "")
        rate = soup.select_one("span#_rate").text.strip()
        icon = "â–²" if "-" not in change else "â–¼"
        return f"â€¢ {name}: {int(price):,}ì› {icon}{change.replace('-', '')} ({rate})"
    except Exception:
        return f"â€¢ {name}: ì •ë³´ ì—†ìŒ"

def fetch_us_market_news_titles():
    try:
        html = http_get("https://finance.yahoo.com/").text
        soup = BeautifulSoup(html, "html.parser")
        arts = soup.select("li.js-stream-content a.js-content-viewer")[:3]
        return "\n".join(
            f"â€¢ {a.get_text(strip=True)}\nğŸ‘‰ {a['href'] if a['href'].startswith('http') else 'https://finance.yahoo.com' + a['href']}"
            for a in arts
        ) or "(ê¸°ì‚¬ ì—†ìŒ)"
    except Exception as e:
        print("[WARN] yahoo fetch failed:", repr(e))
        return "(ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨)"

# â”€â”€ ë„¤ì´ë²„ ë­í‚¹ ë‰´ìŠ¤ (Playwright, íƒ€ì„ì•„ì›ƒ í´ë°±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_media_press_ranking_playwright(press_id="215", count=10):
    url = f"https://media.naver.com/press/{press_id}/ranking"
    result = f"ğŸ“Œ ì–¸ë¡ ì‚¬ {press_id} ë­í‚¹ ë‰´ìŠ¤ TOP {count}\n"
    anchors = []
    with sync_playwright() as p:
        browser = p.chromium.launch(args=["--no-sandbox"])
        page = browser.new_page()
        page.goto(url)
        page.wait_for_load_state("networkidle")
        page.wait_for_timeout(2000)
        try:
            page.wait_for_selector(f"a[href*='/article/{press_id}/']", timeout=10000)
            anchors = page.query_selector_all(f"a[href*='/article/{press_id}/']")[:count]
        except PlaywrightTimeoutError:
            anchors = page.query_selector_all("ul.list_ranking li a")[:count]

        for a in anchors:
            img = a.query_selector("img")
            title = (img.get_attribute("alt").strip() if img and img.get_attribute("alt")
                     else a.inner_text().strip())
            href = (a.get_attribute("href") or "").strip()
            if href and not href.startswith("http"):
                href = "https://n.news.naver.com" + href
            if title:
                result += f"â€¢ {title}\nğŸ‘‰ {href}\n"
        browser.close()
    return result if anchors else f"â€¢ í˜„ì¬ ì‹œì ì— í•´ë‹¹ ì–¸ë¡ ì‚¬ì˜ ë­í‚¹ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

def get_fear_greed_index():
    try:
        j = http_get("https://api.alternative.me/fng/", params={"limit": 1}).json()
        data = j["data"][0]
        value = data["value"]; label = data["value_classification"]
        return f"ğŸ“Œ ê³µí¬Â·íƒìš• ì§€ìˆ˜ (ì½”ì¸ Crypto ê¸°ì¤€): {value}ì  ({label})"
    except Exception as e:
        print("[ERROR] ê³µí¬Â·íƒìš• ì§€ìˆ˜ ì˜ˆì™¸:", e)
        return "ğŸ“Œ ê³µí¬Â·íƒìš• ì§€ìˆ˜: ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨"

# â”€â”€ FRED: CSV ì „ìš© ê²½ë¡œ (API ë¹„í™œì„±í™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fred_csv_latest_combined(series_ids, tries: int = 2):
    base = "https://fred.stlouisfed.org/graph/fredgraph.csv"
    params = {"id": ",".join(series_ids)}
    last_exc = None
    for attempt in range(1, tries + 1):
        try:
            rows = list(csv.DictReader(io.StringIO(http_get(base, params=params).text)))
            latest = {sid: (None, None) for sid in series_ids}
            for row in reversed(rows):
                for sid in series_ids:
                    if latest[sid][1] is not None:
                        continue
                    raw = (row.get(sid) or "").strip()
                    if raw and raw != ".":
                        latest[sid] = (row.get("DATE"), float(raw))
                if all(v[1] is not None for v in latest.values()):
                    break
            missing = [sid for sid, v in latest.items() if v[1] is None]
            if missing:
                raise ValueError(f"No numeric observations for series: {missing}")
            return latest
        except Exception as e:
            last_exc = e
            time.sleep(0.5 * attempt)
    print("[WARN] fredgraph.csv fail:", repr(last_exc))
    return None

def _fred_csv_latest_single(series_id: str, tries: int = 2):
    url = f"https://fred.stlouisfed.org/series/{series_id}/downloaddata/{series_id}.csv"
    last_exc = None
    for attempt in range(1, tries + 1):
        try:
            rows = list(csv.DictReader(io.StringIO(http_get(url).text)))
            for row in reversed(rows):
                raw = (row.get("VALUE") or "").strip()
                if raw and raw != ".":
                    return row.get("DATE"), float(raw)
            raise ValueError(f"No numeric observations for {series_id}")
        except Exception as e:
            last_exc = e
            time.sleep(0.5 * attempt)
    print(f"[WARN] downloaddata CSV fail {series_id}:", repr(last_exc))
    return None

def fred_latest_one(series_id: str, api_key: str | None):
    combo = _fred_csv_latest_combined([series_id])
    if combo and combo.get(series_id) and combo[series_id][1] is not None:
        return combo[series_id]
    return _fred_csv_latest_single(series_id)

# â”€â”€ ë²„í•ì§€ìˆ˜ (CSV + nowcast) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _classify_buffett(pct: float) -> str:
    if pct < 75: return "ì €í‰ê°€ êµ¬ê°„"
    elif pct < 90: return "ì•½ê°„ ì €í‰ê°€"
    elif pct < 115: return "ì ì • ë²”ìœ„"
    elif pct < 135: return "ì•½ê°„ ê³ í‰ê°€"
    else: return "ê³ í‰ê°€ ê²½ê³ "

def fred_value_for_year(series_id: str, year: int):
    url = f"https://fred.stlouisfed.org/series/{series_id}/downloaddata/{series_id}.csv"
    rows = list(csv.DictReader(io.StringIO(http_get(url).text)))
    for row in reversed(rows):
        d = (row.get("DATE") or "")
        v = (row.get("VALUE") or "").strip()
        if d.startswith(f"{year}-") and v and v != ".":
            return d, float(v)
    return None

def _td_get_json(endpoint: str, params: dict, tries: int = 3, sleep_sec: float = 0.8):
    url = f"https://api.twelvedata.com/{endpoint}"
    last = None
    for _ in range(tries):
        try:
            j = http_get(url, params=params).json()
            if isinstance(j, dict) and j.get("status") == "error":
                last = j.get("message"); time.sleep(sleep_sec); continue
            return j
        except Exception as e:
            last = repr(e); time.sleep(sleep_sec)
    raise RuntimeError(f"TwelveData {endpoint} failed: {last}")

def get_vti_latest_and_ref(year: int, api_key: str):
    q = _td_get_json("quote", {"symbol": "VTI", "apikey": api_key})
    latest_str = (q.get("close") or q.get("previous_close") or q.get("price"))
    if latest_str is None:
        raise RuntimeError(f"VTI quote has no price fields: {q}")
    latest = float(latest_str)

    def _fetch_ref(start_date: str, end_date: str):
        ts = _td_get_json("time_series", {
            "symbol": "VTI", "interval": "1day",
            "start_date": start_date, "end_date": end_date,
            "order": "asc", "apikey": api_key,
        })
        vals = ts.get("values") or []
        return float(vals[-1]["close"]) if vals else None

    ref = (_fetch_ref(f"{year}-12-20", f"{year+1}-01-10")
           or _fetch_ref(f"{year}-12-01", f"{year+1}-02-15")
           or _fetch_ref(f"{year}-11-15", f"{year+1}-03-15"))
    if ref is None:
        raise RuntimeError("VTI reference price not found in all windows")

    print(f"[BUFFETT] VTI latest={latest}, base({year} year-end)={ref}")
    return latest, ref

def get_buffett_indicator():
    """
    ğŸ“ ë²„í•ì§€ìˆ˜(í˜„ì¬ ì¶”ì • + ì—°ê°„ í™•ì •)
    - í™•ì •ì¹˜: FRED(World Bank) DDDM01USA156NWDB (%)
    - nowcast: base_pct Ã— (VTI_latest / VTI_base) / (GDP_latest / GDP_base)
    """
    base = fred_latest_one("DDDM01USA156NWDB", FRED_API_KEY)
    if not base:
        print("[WARN] Buffett (DDDM01USA156NWDB) fetch failed")
        return "ğŸ“ ë²„í•ì§€ìˆ˜: ë°ì´í„° ì—†ìŒ"
    base_date, base_pct = base
    base_year = int(base_date[:4]) if base_date else None
    base_line = f"    Â· ì—°ê°„ í™•ì •ì¹˜: {base_pct:.0f}% (ê¸°ì¤€ì—°ë„ {base_year})"

    try:
        gdp_latest = fred_latest_one("GDP", FRED_API_KEY)
        gdp_base   = fred_value_for_year("GDP", base_year)
        if not gdp_latest or not gdp_base:
            raise RuntimeError("GDP ë°ì´í„° ë¶€ì¡±")
        _, gdp_latest_val = gdp_latest
        _, gdp_base_val   = gdp_base

        vti_latest, vti_base = get_vti_latest_and_ref(base_year, TWELVEDATA_API)

        mkt_factor = vti_latest / vti_base
        gdp_factor = gdp_latest_val / gdp_base_val
        nowcast_pct = base_pct * (mkt_factor / gdp_factor)

        print(f"[BUFFETT] GDP latest/base = {gdp_latest_val}/{gdp_base_val} â†’ {gdp_factor:.2f}")
        print(f"[BUFFETT] Nowcast = {base_pct:.1f}% Ã— {mkt_factor:.2f}/{gdp_factor:.2f} = {nowcast_pct:.1f}%")

        head = f"ğŸ“ ë²„í•ì§€ìˆ˜(í˜„ì¬ ì¶”ì •): {nowcast_pct:.0f}% â€” {_classify_buffett(nowcast_pct)}"
        tail = f"{base_line}\n    Â· ë³´ì •ê³„ìˆ˜: VTIÃ—{mkt_factor:.2f} / GDPÃ—{gdp_factor:.2f}"
        return f"{head}\n{tail}"
    except Exception as e:
        print("[WARN] Buffett nowcast failed:", repr(e))
        return f"ğŸ“ ë²„í•ì§€ìˆ˜(ì—°ê°„ í™•ì •ì¹˜): {base_pct:.0f}% â€” {_classify_buffett(base_pct)}\n{base_line}"

# â”€â”€ ë©”ì‹œì§€/ì „ì†¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_message():
    return (
        f"ğŸ“ˆ [{today}] ë‰´ìŠ¤ ìš”ì•½ + ì‹œì¥ ì§€í‘œ\n\n"
        f"ğŸ“Š ë¯¸êµ­ ì£¼ìš” ì§€ìˆ˜:\n{get_us_indices()}\n\n"
        f"ğŸ’± í™˜ìœ¨:\n{get_exchange_rates()}\n\n"
        f"ğŸ“‰ ë¯¸êµ­ ì„¹í„°ë³„ ì§€ìˆ˜ ë³€í™”:\n{get_sector_etf_changes(TWELVEDATA_API)}\n\n"
        f"{get_buffett_indicator()}\n"
        f"{get_fear_greed_index()}\n\n"
        f"{get_stock_prices(TWELVEDATA_API)}\n\n"
        f"ğŸ“° ì„¸ê³„ ì–¸ë¡ ì‚¬ ë­í‚¹ ë‰´ìŠ¤ (press 074):\n{fetch_media_press_ranking_playwright('074', 3)}"
    )



def send_to_telegram():
    part1 = build_message()
    part2 = fetch_media_press_ranking_playwright("215", 10)

    for chat_id in CHAT_IDS:  # âœ… ì—¬ëŸ¬ ëª…ì—ê²Œ ìˆœì°¨ ì „ì†¡
        for msg in (part1, part2):
            if len(msg) > 4000:
                msg = msg[:3990] + "\n(â€» ì¼ë¶€ ìƒëµë¨)"
            res = http_post(TELEGRAM_URL, data={"chat_id": chat_id.strip(), "text": msg})
            print(f"âœ… {chat_id} ì „ì†¡ ì™„ë£Œ | ì½”ë“œ: {res.status_code}")



# â”€â”€ ìŠ¤ì¼€ì¤„ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
schedule.every().day.at("07:00").do(send_to_telegram)
schedule.every().day.at("15:00").do(send_to_telegram)

if __name__ == "__main__":
    send_to_telegram()
